{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "punctuation_list = string.punctuation\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(w_list):\n",
    "     return [word for word in w_list if word not in eng_stopwords]\n",
    "\n",
    "def remove_punctuation(w_list):\n",
    "     return [word for word in w_list if word not in punctuation_list]\n",
    "\n",
    "def remove_number(w_list):\n",
    "     return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def get_pos_tag(tag):\n",
    "     if tag == 'jj':\n",
    "          return 'a'\n",
    "     elif tag in ['nn', 'rb', 'vb']:\n",
    "          return tag[0]\n",
    "     else:\n",
    "          return None\n",
    "\n",
    "def lemmatizing_words(w_list):\n",
    "     lemmatized = []\n",
    "     tagging = pos_tag(w_list)\n",
    "     for word, tag in tagging:\n",
    "          label = get_pos_tag(tag)\n",
    "          if label != None:\n",
    "               lemmatized.append(wnl.lemmatize(word, label))\n",
    "          else:\n",
    "               lemmatized.append(wnl.lemmatize(word))\n",
    "     return lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = remove_stopwords(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_number(words)\n",
    "    words = lemmatizing_words(words)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences, size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences, vector_size=size, window=window, min_count=min_count, workers=workers)\n",
    "    return model\n",
    "\n",
    "def preprocess_for_word2vec(texts):\n",
    "    return [preprocess_text(text).split() for text in texts]\n",
    "\n",
    "def get_average_word2vec(tokens_list, model, k=100):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    vectorized = [model.wv[word] if word in model.wv else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def preprocess_and_vectorize(texts, model):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text).split()\n",
    "        vectors.append(get_average_word2vec(tokens, model))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model():\n",
    "     dataset = pd.read_csv('./dataset_rm.csv')\n",
    "\n",
    "     word_list = []\n",
    "     comments_list = dataset['tweet'].to_list()\n",
    "     label_list = dataset['label'].to_list()\n",
    "\n",
    "     for sentence in comments_list:\n",
    "          words = word_tokenize(sentence)\n",
    "          for word in words:\n",
    "               word_list.append(word)\n",
    "     \n",
    "     word_list = remove_stopwords(word_list)\n",
    "     word_list = remove_punctuation(word_list)\n",
    "     word_list = remove_number(word_list)\n",
    "     # word_list = lemmatizing_words(word_list)\n",
    "\n",
    "     labeled_data = list(zip(word_list, label_list))\n",
    "     fd = FreqDist(word_list)\n",
    "     word_features = [word for word, _ in fd.most_common(n=5000)]\n",
    "     features_sets = []\n",
    "\n",
    "     for comment, label in labeled_data:\n",
    "          features = {}\n",
    "\n",
    "          check_list = word_tokenize(comment)\n",
    "          check_list = remove_stopwords(check_list)\n",
    "          check_list = remove_punctuation(check_list)\n",
    "          check_list = remove_number(check_list)\n",
    "          # check_list = lemmatizing_words(check_list)\n",
    "\n",
    "          for word in word_features:\n",
    "               features[word] = (word in check_list)\n",
    "          features_sets.append((features, label))  \n",
    "     \n",
    "     random.shuffle(features_sets)\n",
    "     train_count = int(len(features_sets)*0.8)\n",
    "     train_dataset = features_sets[:train_count]\n",
    "     test_dataset = features_sets[train_count:]\n",
    "\n",
    "     classifier = NaiveBayesClassifier.train(train_dataset)\n",
    "     classifier.show_most_informative_features(n=10)\n",
    "     print(f\"Training Accuracy: {accuracy(classifier, test_dataset)*100}\")\n",
    "    \n",
    "     file = open('naive_bayes22.pickle','wb')\n",
    "     pickle.dump(classifier, file)\n",
    "     file.close()\n",
    "\n",
    "     y_true = [label for (_, label) in test_dataset]\n",
    "     y_pred = [classifier.classify(features) for (features, _) in test_dataset]\n",
    "\n",
    "     precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "     recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "     f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "     print(f'Precision: {precision:.2f}')\n",
    "     print(f'Recall: {recall:.2f}')\n",
    "     print(f'F1 Score: {f1:.2f}')\n",
    "    \n",
    "     return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13684\\3155786136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13684\\4201539780.py\u001b[0m in \u001b[0;36mtraining_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m           \u001b[0mfeatures_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = training_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  NIGGER = True                1 : 0      =     79.6 : 1.0\n",
      "                   Idiot = True                1 : 0      =     70.5 : 1.0\n",
      "                 Radical = True                1 : 0      =     53.1 : 1.0\n",
      "                feminazi = True                1 : 0      =     45.4 : 1.0\n",
      "                   Miley = True                1 : 0      =     44.5 : 1.0\n",
      "                   Jihad = True                1 : 0      =     43.7 : 1.0\n",
      "                 colored = True                1 : 0      =     43.4 : 1.0\n",
      "                 bullied = True                1 : 0      =     41.8 : 1.0\n",
      "                    Rape = True                1 : 0      =     38.2 : 1.0\n",
      "                    Joke = True                1 : 0      =     37.7 : 1.0\n",
      "Training Accuracy: 66.16666666666666\n",
      "Precision: 0.77\n",
      "Recall: 0.59\n",
      "F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#      print(\"Load model...\")\n",
    "#      file = open('naive_bayes.pickle', 'rb')\n",
    "#      classifier = pickle.load(file)\n",
    "#      file.close()\n",
    "# except:\n",
    "#      print(\"No Model...\")\n",
    "classifier = training_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16eb897c26cdfcf18817bc60a8e0737e3939ff1e8491198c807979170104e811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
