{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.5718888888888889\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57      7660\n",
      "           1       0.67      0.50      0.57     10340\n",
      "\n",
      "    accuracy                           0.57     18000\n",
      "   macro avg       0.58      0.58      0.57     18000\n",
      "weighted avg       0.60      0.57      0.57     18000\n",
      "\n",
      "Naive Bayes Precision: 0.67\n",
      "Naive Bayes Recall: 0.50\n",
      "Naive Bayes F1 Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import string\r\n",
    "import random\r\n",
    "import pickle\r\n",
    "import numpy as np\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.tag import pos_tag\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "eng_stopwords = stopwords.words('english')\r\n",
    "punctuation_list = string.punctuation\r\n",
    "wnl = WordNetLemmatizer()\r\n",
    "\r\n",
    "def remove_stopwords(w_list):\r\n",
    "    return [word for word in w_list if word not in eng_stopwords]\r\n",
    "\r\n",
    "def remove_punctuation(w_list):\r\n",
    "    return [word for word in w_list if word not in punctuation_list]\r\n",
    "\r\n",
    "def remove_number(w_list):\r\n",
    "    return [word for word in w_list if word.isalpha()]\r\n",
    "\r\n",
    "def get_pos_tag(tag):\r\n",
    "    if tag == 'jj':\r\n",
    "        return 'a'\r\n",
    "    elif tag in ['nn', 'rb', 'vb']:\r\n",
    "        return tag[0]\r\n",
    "    else:\r\n",
    "        return None\r\n",
    "\r\n",
    "def lemmatizing_words(w_list):\r\n",
    "    lemmatized = []\r\n",
    "    tagging = pos_tag(w_list)\r\n",
    "    for word, tag in tagging:\r\n",
    "        label = get_pos_tag(tag)\r\n",
    "        if label != None:\r\n",
    "            lemmatized.append(wnl.lemmatize(word, label))\r\n",
    "        else:\r\n",
    "            lemmatized.append(wnl.lemmatize(word))\r\n",
    "    return lemmatized\r\n",
    "\r\n",
    "def preprocess_text(text):\r\n",
    "    words = word_tokenize(text)\r\n",
    "    words = remove_stopwords(words)\r\n",
    "    words = remove_punctuation(words)\r\n",
    "    words = remove_number(words)\r\n",
    "    words = lemmatizing_words(words)\r\n",
    "    return ' '.join(words)\r\n",
    "\r\n",
    "def train_word2vec(sentences, size=100, window=10, min_count=1, workers=5):\r\n",
    "    model = Word2Vec(sentences, vector_size=size, window=window, min_count=min_count, workers=workers)\r\n",
    "    return model\r\n",
    "\r\n",
    "def preprocess_for_word2vec(texts):\r\n",
    "    return [preprocess_text(text).split() for text in texts]\r\n",
    "\r\n",
    "def get_average_word2vec(tokens_list, model, k=100):\r\n",
    "    if len(tokens_list) < 1:\r\n",
    "        return np.zeros(k)\r\n",
    "    vectorized = [model.wv[word] if word in model.wv else np.zeros(k) for word in tokens_list]\r\n",
    "    length = len(vectorized)\r\n",
    "    summed = np.sum(vectorized, axis=0)\r\n",
    "    averaged = np.divide(summed, length)\r\n",
    "    return averaged\r\n",
    "\r\n",
    "def preprocess_and_vectorize(texts, model):\r\n",
    "    vectors = []\r\n",
    "    for text in texts:\r\n",
    "        tokens = preprocess_text(text).split()\r\n",
    "        vectors.append(get_average_word2vec(tokens, model))\r\n",
    "    return np.array(vectors)\r\n",
    "\r\n",
    "def training_model():\r\n",
    "    dataset = pd.read_csv('./dataset_rm.csv')\r\n",
    "\r\n",
    "    comments_list = dataset['tweet'].to_list()\r\n",
    "    label_list = dataset['label'].to_list()\r\n",
    "\r\n",
    "    sentences = preprocess_for_word2vec(comments_list)\r\n",
    "    word2vec_model = train_word2vec(sentences)\r\n",
    "    \r\n",
    "    X_word2vec = preprocess_and_vectorize(comments_list, word2vec_model)\r\n",
    "    y = label_list\r\n",
    "\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "    nb_model = GaussianNB()\r\n",
    "    nb_model.fit(X_train, y_train)\r\n",
    "    y_pred = nb_model.predict(X_test)\r\n",
    "\r\n",
    "    def evaluate_model(y_test, y_pred, model_name):\r\n",
    "        accuracy = accuracy_score(y_test, y_pred)\r\n",
    "        report = classification_report(y_test, y_pred)\r\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\r\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\r\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\r\n",
    "\r\n",
    "        print(f\"{model_name} Accuracy: {accuracy}\")\r\n",
    "        print(f\"{model_name} Classification Report:\\n{report}\")\r\n",
    "        print(f'{model_name} Precision: {precision:.2f}')\r\n",
    "        print(f'{model_name} Recall: {recall:.2f}')\r\n",
    "        print(f'{model_name} F1 Score: {f1:.2f}')\r\n",
    "\r\n",
    "    evaluate_model(y_test, y_pred, \"Naive Bayes\")\r\n",
    "\r\n",
    "    # Save models\r\n",
    "    with open('naive_bayes_word2vec.pickle', 'wb') as file:\r\n",
    "        pickle.dump(nb_model, file)\r\n",
    "\r\n",
    "    with open('word2vec_model.pickle', 'wb') as file:\r\n",
    "        pickle.dump(word2vec_model, file)\r\n",
    "    \r\n",
    "    return nb_model, word2vec_model\r\n",
    "\r\n",
    "model, word2vec_model = training_model()\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13427881f06f13311079f5221e5dd632fdf9146891f6da22d47a93dcb9272d3a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}